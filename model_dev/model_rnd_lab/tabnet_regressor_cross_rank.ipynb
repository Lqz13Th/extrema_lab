{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9842c04a-8719-4681-9d25-f034092004b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已添加项目根目录到 sys.path: C:\\quant\\work\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def add_project_root(project_name: str = \"extrema_lab\"):\n",
    "    cwd = os.getcwd()\n",
    "    path_parts = cwd.split(os.sep)\n",
    "\n",
    "    for i in range(len(path_parts), 0, -1):\n",
    "        potential_root = os.sep.join(path_parts[:i])\n",
    "        if os.path.basename(potential_root) == project_name:\n",
    "            root = os.path.dirname(potential_root)\n",
    "            if root not in sys.path:\n",
    "                sys.path.append(root)\n",
    "            print(f\"已添加项目根目录到 sys.path: {root}\")\n",
    "            return\n",
    "\n",
    "    print(f\"未找到项目 {project_name}，请确认路径是否正确\")\n",
    "\n",
    "\n",
    "add_project_root(\"extrema_lab\")\n",
    "from extrema_lab.feature_eng.operator.utils_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4127ec8a-7b0b-4fc7-ab80-862ce9a9e16e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "系统找不到指定的文件。 (os error 2): C:\\quant\\work\\extrema_lab\\data_proc\\resampled_data\\AXSUSDT_merged_thr0.0067.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m\n\u001b[0;32m     13\u001b[0m default_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0067\u001b[39m\n\u001b[0;32m     15\u001b[0m symbol_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m     sym: {\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m0.0031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sym \u001b[38;5;129;01min\u001b[39;00m special_tokens \u001b[38;5;28;01melse\u001b[39;00m default_threshold),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sym \u001b[38;5;129;01min\u001b[39;00m symbols_list_usdt\n\u001b[0;32m     23\u001b[0m }\n\u001b[1;32m---> 25\u001b[0m symbol_dfs \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_all_symbols\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m long_df \u001b[38;5;241m=\u001b[39m build_long_cross_sections_fast(symbol_dfs)\n",
      "File \u001b[1;32mC:\\quant\\work\\extrema_lab\\feature_eng\\operator\\utils_tools.py:163\u001b[0m, in \u001b[0;36mprocess_all_symbols\u001b[1;34m(params_dict)\u001b[0m\n\u001b[0;32m    161\u001b[0m symbol_dfs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sym, param \u001b[38;5;129;01min\u001b[39;00m params_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 163\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_single_symbol\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43msymbol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msym\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreshold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeat_cal_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeat_cal_window\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeat_norm_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeat_norm_window\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeat_norm_rolling_mean_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeat_norm_rolling_mean_window\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mlit(sym)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymbol\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    171\u001b[0m     symbol_dfs[sym] \u001b[38;5;241m=\u001b[39m df\n",
      "File \u001b[1;32mC:\\quant\\work\\extrema_lab\\feature_eng\\operator\\utils_tools.py:146\u001b[0m, in \u001b[0;36mprocess_single_symbol\u001b[1;34m(symbol, threshold, feat_cal_window, feat_norm_window, feat_norm_rolling_mean_window, data_dir)\u001b[0m\n\u001b[0;32m    144\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_merged_thr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, file)\n\u001b[1;32m--> 146\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m df \u001b[38;5;241m=\u001b[39m cal_factors_with_sampled_data(df, feat_cal_window)\n\u001b[0;32m    148\u001b[0m df \u001b[38;5;241m=\u001b[39m rolling_z_tanh_normalize(df, feat_norm_window, feat_norm_rolling_mean_window)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EI_Lab\\lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    125\u001b[0m     _rename_keyword_argument(\n\u001b[0;32m    126\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[0;32m    127\u001b[0m     )\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EI_Lab\\lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    125\u001b[0m     _rename_keyword_argument(\n\u001b[0;32m    126\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, version\n\u001b[0;32m    127\u001b[0m     )\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EI_Lab\\lib\\site-packages\\polars\\io\\parquet\\functions.py:289\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(source, columns, n_rows, row_index_name, row_index_offset, parallel, use_statistics, hive_partitioning, glob, schema, hive_schema, try_parse_hive_dates, rechunk, low_memory, storage_options, credential_provider, retries, use_pyarrow, pyarrow_options, memory_map, include_file_paths, missing_columns, allow_missing_columns)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m         lf \u001b[38;5;241m=\u001b[39m lf\u001b[38;5;241m.\u001b[39mselect(columns)\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EI_Lab\\lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[0m, in \u001b[0;36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min-memory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EI_Lab\\lib\\site-packages\\polars\\lazyframe\\opt_flags.py:330\u001b[0m, in \u001b[0;36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m         optflags \u001b[38;5;241m=\u001b[39m cb(optflags, kwargs\u001b[38;5;241m.\u001b[39mpop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[0;32m    329\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizations\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m optflags\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\EI_Lab\\lib\\site-packages\\polars\\lazyframe\\frame.py:2335\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[1;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[0m\n\u001b[0;32m   2333\u001b[0m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[0;32m   2334\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[1;32m-> 2335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: 系统找不到指定的文件。 (os error 2): C:\\quant\\work\\extrema_lab\\data_proc\\resampled_data\\AXSUSDT_merged_thr0.0067.parquet\n\nThis error occurred with the following context stack:\n\t[1] 'parquet scan'\n\t[2] 'sink'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"../../symbols.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    symbols_list = json.load(f)\n",
    "\n",
    "symbols_list_usdt = [s if s.endswith(\"T\") else s + \"T\" for s in symbols_list]\n",
    "\n",
    "feat_cal_window = 5000\n",
    "feat_norm_window = 2000\n",
    "feat_norm_rolling_mean_window = 500\n",
    "\n",
    "special_tokens = {\"BTCUSDT\", \"BNBUSDT\"}\n",
    "\n",
    "default_threshold = 0.0067\n",
    "\n",
    "symbol_params = {\n",
    "    sym: {\n",
    "        \"threshold\": str(0.0031 if sym in special_tokens else default_threshold),\n",
    "        \"feat_cal_window\": feat_cal_window,\n",
    "        \"feat_norm_window\": feat_norm_window,\n",
    "        \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "    }\n",
    "    for sym in symbols_list_usdt\n",
    "}\n",
    "\n",
    "symbol_dfs = process_all_symbols(symbol_params)\n",
    "long_df = build_long_cross_sections_fast(symbol_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee66d7-7e7f-4652-b633-b18501853f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "del symbol_dfs  # 删除变量引用\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d788459-8b72-4d51-90c7-9f4ee68efef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(long_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0d3aa-e239-4795-8ea2-1384632aaa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "from pytorch_tabnet.callbacks import History\n",
    "from pytorch_tabnet.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e7fa82-91bf-48ef-9c14-2c55b27192cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3000\n",
    "target_col = f\"future_return_{N}\"\n",
    "exclude_prefixes = ['px', 'timestamp', 'timestamp_dt', 'symbol']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb496c-5a18-4a4f-837a-6dca9644d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "le_symbol = LabelEncoder()\n",
    "long_df = long_df.with_columns(pl.col('symbol').str.to_uppercase())\n",
    "\n",
    "all_symbols = set()\n",
    "all_symbols.update(long_df[\"symbol\"].unique())\n",
    "le_symbol.fit(sorted(list(all_symbols)))\n",
    "\n",
    "symbol_encoded = le_symbol.transform(long_df['symbol'].to_list())\n",
    "long_df = long_df.with_columns([pl.Series('enc_cat_symbol', symbol_encoded)])\n",
    "\n",
    "symbol_to_id = dict(zip(le_symbol.classes_, le_symbol.transform(le_symbol.classes_)))\n",
    "id_to_symbol = {v: k for k, v in symbol_to_id.items()}\n",
    "\n",
    "# 加 row_nr\n",
    "long_temp_df = long_df.with_row_index(name=\"row_nr\")\n",
    "del long_df\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82225107-459e-44c7-95b8-d1ab8fb201b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_future = (\n",
    "    long_temp_df.sort([\"symbol\", \"timestamp\"])\n",
    "    .group_by(\"symbol\", maintain_order=True)\n",
    "    .map_groups(lambda g: g.with_columns([\n",
    "        pl.col(\"px\").shift(-N).alias(\"px_future\"),\n",
    "        (pl.col(\"px\").shift(-N) / pl.col(\"px\")).log().alias(f\"future_return_{N}\")\n",
    "    ]))\n",
    "    .sort(\"row_nr\")\n",
    "    .drop(\"row_nr\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81d97e-c7e0-4341-84cb-25e5f3237337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 long format 下，按 timestamp 做截面标准化 & 排序\n",
    "df_with_future = df_with_future.with_columns([\n",
    "    # 截面 z-score\n",
    "    ((pl.col(c) - pl.col(c).mean().over(\"timestamp\")) /\n",
    "     pl.when(pl.col(c).std().over(\"timestamp\") > 1e-9)\n",
    "       .then(pl.col(c).std().over(\"timestamp\"))\n",
    "       .otherwise(1)\n",
    "    ).alias(f\"{c}_zscore_cs\")\n",
    "    for c in time_series_feature_cols\n",
    "])# + [\n",
    "#     # 截面 rank（归一化到 [0,1]）\n",
    "#     (pl.col(c).rank(\"average\").over(\"timestamp\") /\n",
    "#      pl.len().over(\"timestamp\")).alias(f\"{c}_rank_cs\")\n",
    "#     for c in time_series_feature_cols\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4fcb2-0751-41e7-979a-d2f2e5ded010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 enc_cat_symbol 移到最后一列\n",
    "cols = [c for c in df_with_future.columns if c != \"enc_cat_symbol\"] + [\"enc_cat_symbol\"]\n",
    "df_with_future = df_with_future.select(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a26a29-0e70-4fab-aa5b-37bee2316928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_future = clean_df_drop_nulls(df_with_future)\n",
    "split_dataframes = split_df_by_month(df_with_future)  # 只拿 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad334fe0-3419-4c13-9783-668a23632573",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(symbol_to_id) \n",
    "print(id_to_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf8b9d-de29-4b8d-a44e-7039226b03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_dataframes[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629772e1-e36a-4736-ba61-6342eb764e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用第一个 df 定义 feature_cols\n",
    "sample_df = split_dataframes[0]\n",
    "\n",
    "cat_idxs = [feature_cols.index('enc_cat_symbol')]\n",
    "cat_dims = [sample_df.select('enc_cat_symbol').n_unique()]\n",
    "cat_emb_dim = 16\n",
    "print(len(feature_cols), cat_idxs, cat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f9762c-285f-44f4-8341-f171c06e6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_cross_section(symbols, y_true, y_binary, y_pred_prob, px, alpha=1.0):\n",
    "    x = np.arange(len(symbols))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    width = 0.2\n",
    "\n",
    "    # 真实未来收益（连续）\n",
    "    ax1.bar(x - width, y_true, width=width, label='Future Return', alpha=0.6)\n",
    "    ax1.set_ylabel('Future Return')\n",
    "\n",
    "    # 价格线\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, px, label='Price', color='tab:blue', marker='o')\n",
    "    ax2.set_ylabel('Price')\n",
    "\n",
    "    # 分类标签（二分类）\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines.right.set_position((\"outward\", 60))\n",
    "    ax3.scatter(x, y_binary, label='GMM Label', color='tab:orange', marker='x')\n",
    "    ax3.set_ylim(-0.1, 1.1)\n",
    "    ax3.set_ylabel('Binary Label')\n",
    "\n",
    "    # 预测概率\n",
    "    ax4 = ax1.twinx()\n",
    "    ax4.spines.right.set_position((\"outward\", 120))\n",
    "    ax4.plot(x, y_pred_prob, label='Predicted Prob', color='tab:green', marker='^')\n",
    "    ax4.set_ylim(-0.05, 1.05)\n",
    "    ax4.set_ylabel('Predicted Probability')\n",
    "\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(symbols, rotation=45)\n",
    "    ax1.set_xlabel('Symbols')\n",
    "\n",
    "    # 合并图例\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    lines_3, labels_3 = ax3.get_legend_handles_labels()\n",
    "    lines_4, labels_4 = ax4.get_legend_handles_labels()\n",
    "\n",
    "    ax1.legend(\n",
    "        lines_1 + lines_2 + lines_3 + lines_4,\n",
    "        labels_1 + labels_2 + labels_3 + labels_4,\n",
    "        loc='upper left'\n",
    "    )\n",
    "\n",
    "    plt.title(\"Cross-Section Comparison at One Timestamp\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67895f48-f727-4775-b1dc-966f164601c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinEpochsEarlyStopping(EarlyStopping):\n",
    "    def __init__(self, early_stopping_metric, patience, min_epochs=5, is_maximize=False, tol=0.0):\n",
    "        super().__init__(\n",
    "            early_stopping_metric=early_stopping_metric,\n",
    "            patience=patience,\n",
    "            is_maximize=is_maximize,\n",
    "            tol=tol\n",
    "        )\n",
    "        self.min_epochs = min_epochs\n",
    "        self._callback_reset_flag = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # 在 min_epochs 之前，不触发早停逻辑\n",
    "        if epoch < self.min_epochs:\n",
    "            return\n",
    "\n",
    "        self.stopped_epoch = epoch\n",
    "        \n",
    "        # 调用父类逻辑继续正常早停检查\n",
    "        super().on_epoch_end(max(epoch, self.min_epochs), logs)\n",
    "        \n",
    "        # 第一次达到 min_epochs 时重置 baseline\n",
    "        if not self._callback_reset_flag:\n",
    "            print(f\"[Check] Current best_epoch={self.best_epoch}, best_loss={self.best_loss:.6f}\")\n",
    "\n",
    "            if self.early_stopping_metric not in logs:\n",
    "                raise KeyError(f\"Metric '{self.early_stopping_metric}' not found in logs keys={list(logs.keys())}\")\n",
    "            \n",
    "            self.best_loss = logs[self.early_stopping_metric]\n",
    "            self.best_epoch = epoch\n",
    "\n",
    "            # preventing double log bug\n",
    "            self._callback_reset_flag = True\n",
    "            print(f\"[MinEpochsEarlyStopping] Reset best_score at epoch {epoch} to {self.best_loss:.6f}\")\n",
    "\n",
    "        print(f\"[DEBUG] After super(): best_loss={self.best_loss:.6f}, best_epoch={self.best_epoch}, wait={self.wait}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610df1ae-0393-456d-b2e5-064ac811886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_weeks = 8 # 可配置\n",
    "n_val_weeks = 1    # 一般 1 周验证\n",
    "n_test_weeks = 1   # 后 1 周做 test\n",
    "\n",
    "tabnet = None\n",
    "\n",
    "all_preds = []  # 放到 for 循环外\n",
    "\n",
    "for i in range(len(weekly_dataframes) - n_train_weeks - n_val_weeks - n_test_weeks + 1):\n",
    "    train_dfs = weekly_dataframes[i : i + n_train_weeks]\n",
    "    val_dfs = weekly_dataframes[i + n_train_weeks : i + n_train_weeks + n_val_weeks]\n",
    "    test_dfs = weekly_dataframes[i + n_train_weeks + n_val_weeks : i + n_train_weeks + n_val_weeks + n_test_weeks]\n",
    "\n",
    "    train_df = pl.concat(train_dfs)\n",
    "    val_df = pl.concat(val_dfs)\n",
    "    test_df = pl.concat(test_dfs)\n",
    "    \n",
    "    def process_df_np(df):\n",
    "        df = df.sort('timestamp').drop_nulls(subset=feature_cols + [target_col, 'px'])\n",
    "        X = df.select(feature_cols).to_numpy()  # Polars DataFrame 转 numpy ndarray\n",
    "        y = df.select(target_col).to_numpy().reshape(-1, 1)\n",
    "        px = df.select('px').to_numpy()\n",
    "        ts = df.select('timestamp').to_numpy()\n",
    "        symbol_enc = df.select(\"enc_cat_symbol\")\n",
    "        return X, y, px, ts, symbol_enc\n",
    "\n",
    "    X_train, y_train, px_train, ts_train, sb_train = process_df_np(train_df)\n",
    "    X_val, y_val, px_val, ts_val, sb_val = process_df_np(val_df)\n",
    "    X_test, y_test, px_test, ts_test, sb_test = process_df_np(test_df)\n",
    "\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Fold {i}: Train {i}~{i+n_train_weeks-1}, Val {i+n_train_weeks}, Test {i+n_train_weeks+1}\")\n",
    "    print(\"Train:\", train_df['timestamp_dt'][0], \"to\", train_df['timestamp_dt'][-1])\n",
    "    print(\"Val:\", val_df['timestamp_dt'][0], \"to\", val_df['timestamp_dt'][-1])\n",
    "    print(\"Test:\", test_df['timestamp_dt'][0], \"to\", test_df['timestamp_dt'][-1])\n",
    "    \n",
    "\n",
    "    params = {\n",
    "        # 模型结构参数\n",
    "        \"n_d\": 8,                      # 决策输出维度\n",
    "        \"n_a\": 8,                      # 注意力机制维度\n",
    "        \"n_steps\": 3,                  # 决策步数\n",
    "        \"gamma\": 1.3,                  # 控制特征复用的程度（>1）\n",
    "        \"n_independent\": 3,           # 每个 step 的独立 Feature Transformer 层数\n",
    "        \"n_shared\": 2,                # 每个 step 的共享 Feature Transformer 层数\n",
    "    \n",
    "        # 分类特征嵌入（如果你用的都是 float 特征，可以全留空）\n",
    "        \"cat_idxs\": cat_idxs,               # 类别特征的列索引\n",
    "        \"cat_dims\": cat_dims,               # 每个类别特征的类别数\n",
    "        \"cat_emb_dim\": cat_emb_dim,             # 类别特征的嵌入维度（或 list）\n",
    "    \n",
    "        # 正则化与数值稳定性\n",
    "        \"lambda_sparse\": 1e-5,        # 稀疏正则\n",
    "        \"epsilon\": 1e-15,             # sparsemax 稳定项\n",
    "        \"momentum\": 0.03,             # BatchNorm 的动量\n",
    "        \"clip_value\": 3.0,            # 梯度裁剪\n",
    "        \n",
    "        # 注意力 mask 类型\n",
    "        \"mask_type\": \"sparsemax\",     # sparsemax 或 entmax\n",
    "    \n",
    "        # 优化器设置（函数和参数）\n",
    "        # \"optimizer_fn\": torch.optim.Adam,    \n",
    "        \"optimizer_params\": {\"lr\": 5e-5},\n",
    "    \n",
    "        # 学习率调度器（可选）\n",
    "        \"scheduler_fn\": None,         # torch.optim.lr_scheduler.StepLR 等\n",
    "        \"scheduler_params\": {},       # 比如 {\"step_size\": 20, \"gamma\": 0.95}\n",
    "    \n",
    "        # 预训练解码器结构（一般用不到）\n",
    "        \"n_shared_decoder\": 1,\n",
    "        \"n_indep_decoder\": 1,\n",
    "    \n",
    "        # 训练环境和调试\n",
    "        \"seed\": 7,\n",
    "        \"verbose\": 2,\n",
    "        \"device_name\": \"cuda\",        # auto / cpu / cuda\n",
    "    }\n",
    "\n",
    "    init_fit_params = {\n",
    "        \"eval_metric\": ['mae'],\n",
    "        \"max_epochs\": 20,\n",
    "        # \"patience\": 5,\n",
    "        \"batch_size\": 2048,\n",
    "        \"virtual_batch_size\": 512,\n",
    "        \"compute_importance\": False,\n",
    "    }\n",
    "\n",
    "    my_early_stopping = MinEpochsEarlyStopping(\n",
    "        early_stopping_metric='val_0_mae',\n",
    "        patience=3,\n",
    "        min_epochs=3,\n",
    "    )\n",
    "    \n",
    "    tabnet = TabNetRegressor(**params )\n",
    "    tabnet.fit(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        callbacks=[my_early_stopping], # 使用你的自定义回调\n",
    "        **init_fit_params,\n",
    "    )\n",
    "\n",
    "    y_pred = tabnet.predict(X_test).squeeze()\n",
    "    print(ts_test.shape, y_test.shape, y_pred.shape, px_test.shape)\n",
    "\n",
    "    print(f\"MSE: {mean_squared_error(y_test, y_pred):.6f}\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, y_pred):.6f}\")\n",
    "    current_window_results = {\n",
    "        'timestamp': ts_test,\n",
    "        'symbol_enc': sb_test, # 收集价格，回测时需要\n",
    "        'true_label': y_test,\n",
    "        'predicted_prob': y_pred,\n",
    "        'px': px_test, # 收集价格，回测时需要\n",
    "    }\n",
    "    \n",
    "    all_preds.append(current_window_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8eaed-ee4a-4ded-bdd3-3d6f27d1bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672ab25-73ff-4e23-a7b3-ae9268887248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"all_preds length: {len(all_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f5194-975c-42fb-b80a-72e7ee0cf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_cross_section_comparison(symbols, true_labels, pred_probs, prices, std_array=None, alpha=1):\n",
    "    \"\"\"\n",
    "    symbols: list/array of symbol names或编码（横轴）\n",
    "    true_labels: array，对应每个币种的真实标签\n",
    "    pred_probs: array，对应每个币种的预测概率\n",
    "    prices: array，对应每个币种的价格\n",
    "    std_array: array，可选，价格的波动区间\n",
    "    alpha: 标准差放大倍数\n",
    "    \"\"\"\n",
    "    x = np.arange(len(symbols))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "    # 真实标签（可以用点图）\n",
    "    ax1.scatter(x, true_labels, label=\"True Label\", color='tab:blue', marker='o', s=50, alpha=0.7)\n",
    "    ax1.set_ylabel(\"True Label\", color='tab:blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.set_ylim(min(true_labels)*1.1, max(true_labels)*1.1)\n",
    "\n",
    "    # 预测概率\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, pred_probs, label=\"Predicted Probability\", color='tab:green', marker='x', linestyle='-', alpha=0.7)\n",
    "    ax2.set_ylabel(\"Predicted Probability\", color='tab:green')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    # 价格\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines.right.set_position((\"outward\", 60))\n",
    "    ax3.plot(x, prices, label=\"Price\", color='tab:red', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # 价格区间带\n",
    "    if std_array is not None:\n",
    "        ax3.fill_between(x, prices - alpha * std_array, prices + alpha * std_array,\n",
    "                         color='tab:red', alpha=0.15, label=\"Price ± std\")\n",
    "\n",
    "    ax3.set_ylabel(\"Price\", color='tab:red')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    # 横轴是币种名\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(symbols, rotation=45, ha='right')\n",
    "\n",
    "    plt.title(\"Cross-Sectional Comparison: True Label, Prediction & Price\")\n",
    "    # 合并图例\n",
    "    lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "    lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "    lines_3, labels_3 = ax3.get_legend_handles_labels()\n",
    "    ax1.legend(lines_1 + lines_2 + lines_3, labels_1 + labels_2 + labels_3, loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ead00-2dc1-4c50-a427-7d86530f4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_list = []\n",
    "for i, result in enumerate(all_preds):\n",
    "    try:\n",
    "        # 先处理 symbol_enc 转成 numpy array\n",
    "        # 假设 result['symbol_enc'] 是 Polars DataFrame，列名是 'enc_cat_symbol'\n",
    "        if hasattr(result['symbol_enc'], \"to_pandas\"):\n",
    "            symbol_enc_array = result['symbol_enc'].to_pandas()['enc_cat_symbol'].values\n",
    "        else:\n",
    "            # 如果已经是 np.ndarray 或 list\n",
    "            symbol_enc_array = result['symbol_enc']\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'timestamp': result['timestamp'].squeeze(),  # (N,)\n",
    "            'symbol_enc': symbol_enc_array.squeeze(),    # (N,)\n",
    "            'true_label': result['true_label'].squeeze(),\n",
    "            'predicted_prob': result['predicted_prob'].squeeze(),\n",
    "            'px': result['px'].squeeze()\n",
    "        })\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"Warning: Empty dataframe at index {i}\")\n",
    "            continue\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "\n",
    "full_df = pd.concat(df_list, ignore_index=True)\n",
    "print(full_df.shape)\n",
    "print(full_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a930cbc-eb36-4af2-9dc8-eff1df709d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907eb19-b766-4817-b324-c013af4d6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_analysis(factor_name, weekly_dataframes, target_col, num_bins=5):\n",
    "    bin_returns = [0.0] * num_bins\n",
    "    bin_counts = [0] * num_bins\n",
    "\n",
    "    for df in weekly_dataframes:\n",
    "        if factor_name not in df.columns or target_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        sub_df = df.select([factor_name, target_col]).drop_nulls().to_pandas()\n",
    "        if len(sub_df) < num_bins:\n",
    "            continue\n",
    "\n",
    "        sub_df[\"bin\"] = pd.qcut(sub_df[factor_name], q=num_bins, labels=False, duplicates=\"drop\")\n",
    "        for i in range(num_bins):\n",
    "            group = sub_df[sub_df[\"bin\"] == i]\n",
    "            if not group.empty:\n",
    "                bin_returns[i] += group[target_col].mean()\n",
    "                bin_counts[i] += 1\n",
    "\n",
    "    avg_returns = [r / c if c > 0 else 0 for r, c in zip(bin_returns, bin_counts)]\n",
    "    return avg_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f2246-5b71-4ecd-96b2-f4a872f29329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def calc_ic_per_factor(weekly_dataframes, feature_cols, target_col):\n",
    "    ic_records = []\n",
    "\n",
    "    for feature in tqdm(feature_cols, unit=\"factor\"):\n",
    "        \n",
    "        ic_list = []\n",
    "        for df in weekly_dataframes:\n",
    "            if feature not in df.columns or target_col not in df.columns:\n",
    "                continue\n",
    "\n",
    "            sub_df = df.select([feature, target_col]).drop_nulls().to_pandas()\n",
    "            if len(sub_df) < 5:\n",
    "                continue\n",
    "\n",
    "            rank_ic, _ = spearmanr(sub_df[feature], sub_df[target_col])\n",
    "            if pd.notna(rank_ic):\n",
    "                ic_list.append(rank_ic)\n",
    "\n",
    "        if ic_list:\n",
    "            ic_mean = sum(ic_list) / len(ic_list)\n",
    "            ic_std = pd.Series(ic_list).std()\n",
    "            ic_ir = ic_mean / ic_std if ic_std > 1e-6 else 0\n",
    "            ic_records.append({\n",
    "                'factor': feature,\n",
    "                'IC Mean': ic_mean,\n",
    "                'IC Std': ic_std,\n",
    "                'IC IR': ic_ir\n",
    "            })\n",
    "\n",
    "    ic_df = pd.DataFrame(ic_records).sort_values(by='IC IR', ascending=False)\n",
    "    return ic_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aad97c-b426-406e-8183-bade16a439e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_df = calc_ic_per_factor(weekly_dataframes, feature_cols, target_col)\n",
    "print(\"Top10 IC 因子：\")\n",
    "print(ic_df.head(10))\n",
    "\n",
    "top_factors = ic_df.head(10)['factor'].tolist()\n",
    "for factor in top_factors:\n",
    "    returns = bin_analysis(factor, weekly_dataframes, target_col, num_bins=50)\n",
    "    print(f\"Factor: {factor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71afe7-889a-4a6e-95a5-c5312edeed16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in ic_df.iterrows():\n",
    "    factor = row['factor']\n",
    "    ic = row['IC Mean']\n",
    "    returns = bin_analysis(factor, weekly_dataframes, target_col, num_bins=50)\n",
    "    print(f\"Factor: {factor}, IC: {ic:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dde81a-7657-41c3-b9cd-d67f5958f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ic_df)\n",
    "tail_factors = ic_df.tail(10)['factor'].tolist()\n",
    "for factor in tail_factors:\n",
    "    returns = bin_analysis(factor, weekly_dataframes, target_col, num_bins=50)\n",
    "    print(f\"Factor: {factor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9e2df-47df-4cbe-84d1-722be21b7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, factor in enumerate(top_factors):\n",
    "    returns = bin_analysis(factor, weekly_dataframes, target_col, num_bins=50)\n",
    "    axes[i].bar(range(1, 1 + len(returns)), returns)\n",
    "    axes[i].set_title(f\"Factor: {factor}\")\n",
    "    axes[i].set_xlabel(\"Bin\")\n",
    "    axes[i].set_ylabel(\"Mean Return\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ca16d-81c1-4fbd-8ac1-4aa2aae62aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, factor in enumerate(tail_factors):\n",
    "    returns = bin_analysis(factor, weekly_dataframes, target_col, num_bins=50)\n",
    "    axes[i].bar(range(1, 1 + len(returns)), returns)\n",
    "    axes[i].set_title(f\"Factor: {factor}\")\n",
    "    axes[i].set_xlabel(\"Bin\")\n",
    "    axes[i].set_ylabel(\"Mean Return\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623bc07b-614c-40ef-83cf-6b747f590081",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"总样本数：\", len(full_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f3743c-5845-4896-b106-3f1f22ef9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设你已有 df，包含 symbol_enc、timestamp、true_label、predicted_prob、px、position（自行添加）\n",
    "# 转换 timestamp 为时间格式（如需要）\n",
    "df = full_df\n",
    "df[\"timestamp\"] = pd.to_datetime(full_df[\"timestamp\"], unit=\"ms\")\n",
    "\n",
    "# 遍历每个币种\n",
    "symbols = df[\"symbol_enc\"].unique()\n",
    "fig, axs = plt.subplots(len(symbols), 1, figsize=(14, 3 * len(symbols)), sharex=True)\n",
    "\n",
    "if len(symbols) == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "for i, sym in enumerate(symbols):\n",
    "    sym_df = df[df[\"symbol_enc\"] == sym].copy()\n",
    "    \n",
    "    ax = axs[i]\n",
    "    sym_str = id_to_symbol[int(sym)]\n",
    "    \n",
    "    ax.set_title(f\"{sym_str}\")\n",
    "    \n",
    "    # 主轴: 价格\n",
    "    ax.plot(sym_df[\"timestamp\"], sym_df[\"px\"], label=\"Price\", color=\"black\")\n",
    "    ax.set_ylabel(\"Price\", color=\"black\")\n",
    "    ax.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "    # 第二轴: label\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(sym_df[\"timestamp\"], sym_df[\"true_label\"], label=\"Label\", color=\"blue\", alpha=0.6)\n",
    "    ax2.set_ylabel(\"Label\", color=\"blue\")\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # 第三轴: predicted prob\n",
    "    ax3 = ax.twinx()\n",
    "    ax3.spines.right.set_position((\"axes\", 1.1))  # 偏移右边\n",
    "    ax3.plot(sym_df[\"timestamp\"], sym_df[\"predicted_prob\"], label=\"Pred Prob\", color=\"orange\", alpha=0.6)\n",
    "    ax3.set_ylabel(\"Predicted\", color=\"orange\")\n",
    "    ax3.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "    # 第四轴: position（如果你有这个字段）\n",
    "    if \"position\" in sym_df.columns:\n",
    "        ax4 = ax.twinx()\n",
    "        ax4.spines.right.set_position((\"axes\", 1.2))  # 更偏移右边\n",
    "        ax4.plot(sym_df[\"timestamp\"], sym_df[\"position\"], label=\"Position\", color=\"green\", alpha=0.6)\n",
    "        ax4.set_ylabel(\"Position\", color=\"green\")\n",
    "        ax4.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881039c-a5c0-4ae6-a1b3-2cff520f7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "ic_list = []\n",
    "time_list = []\n",
    "\n",
    "# 计算每个时间截面的 IC\n",
    "for ts, group in tqdm(full_df.groupby('timestamp'), total=full_df['timestamp'].nunique(), desc=\"Calculating IC\"):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "    if group['predicted_prob'].nunique() < 2 or group['true_label'].nunique() < 2:\n",
    "        continue\n",
    "    ic, _ = spearmanr(group['predicted_prob'], group['true_label'])\n",
    "    ic_list.append(ic)\n",
    "    time_list.append(ts)\n",
    "\n",
    "# 转为 np.array 和 datetime 格式（如需要）\n",
    "ic_array = np.array(ic_list)\n",
    "time_array = np.array(time_list)\n",
    "\n",
    "# 排序时间（可选，保险做法）\n",
    "sorted_idx = np.argsort(time_array)\n",
    "time_array = time_array[sorted_idx]\n",
    "ic_array = ic_array[sorted_idx]\n",
    "\n",
    "# 计算累计 IC（cum IC）\n",
    "cum_ic = np.cumsum(ic_array)\n",
    "\n",
    "# 计算 IR（信息比率 = 平均IC / IC标准差）\n",
    "ir = np.mean(ic_array) / np.std(ic_array)\n",
    "\n",
    "# 打印信息\n",
    "print(f\"平均IC: {np.mean(ic_array):.4f}\")\n",
    "print(f\"平均IC std: {np.std(ic_array):.4f}\")\n",
    "\n",
    "print(f\"信息比率 IR: {ir:.4f}\")\n",
    "\n",
    "# 每隔500点采样一次\n",
    "step = 500\n",
    "time_array_sampled = time_array[::step]\n",
    "ic_array_sampled = ic_array[::step]\n",
    "cum_ic_sampled = cum_ic[::step]\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# 子图1: IC 时间序列图（采样后）\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time_array_sampled, ic_array_sampled, marker='o', label='IC')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.title(\"Information Coefficient (IC) over Time (sampled every 500 points)\")\n",
    "plt.ylabel(\"IC\")\n",
    "plt.legend()\n",
    "\n",
    "# 子图2: 累积 IC 图（采样后）\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time_array_sampled, cum_ic_sampled, color='orange', label='Cumulative IC')\n",
    "plt.title(f\"Cumulative IC (IR={ir:.4f})\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Cumulative IC\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c27ab-9ee9-46ca-bbeb-682366eba70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # 用于显示进度条\n",
    "\n",
    "# --- 假设 full_df 已经包含以下列 ---\n",
    "# - 'timestamp': 原始时间戳（例如毫秒），唯一且递增\n",
    "# - 'symbol_enc': 币种编码\n",
    "# - 'px': 当期价格\n",
    "# - 'predicted_label': 模型预测的标签（分类或排名），越大表示越好\n",
    "\n",
    "# --- 配置参数 ---\n",
    "N_INTERVAL = N # 调仓周期，每 N_INTERVAL 个时间戳调仓一次\n",
    "\n",
    "# N_INTERVAL = 1000 # 调仓周期，每 N_INTERVAL 个时间戳调仓一次\n",
    "\n",
    "# --- 数据预处理 ---\n",
    "# 1. 转换时间戳为 datetime 类型，方便处理和绘图\n",
    "bt_df = full_df\n",
    "bt_df['dt'] = pd.to_datetime(full_df['timestamp'], unit='ms')\n",
    "\n",
    "# 2. 确保数据按时间戳和币种排序，这是后续分组和shift操作的基础\n",
    "bt_df = bt_df.sort_values(['dt', 'symbol_enc']).reset_index(drop=True)\n",
    "\n",
    "# 3. 获取所有唯一的、排序后的时间戳列表\n",
    "timestamps_sorted = bt_df['dt'].drop_duplicates().sort_values().to_list()\n",
    "\n",
    "# 4. 确定所有调仓时间点\n",
    "rebalance_times = timestamps_sorted[::N_INTERVAL]\n",
    "\n",
    "# --- 识别多空信号（在每个调仓时间点）---\n",
    "# 记录每个调仓时间点应持有的多空币种\n",
    "rebalance_signals = {}\n",
    "for t in tqdm(rebalance_times, desc=\"Identifying Rebalance Signals\"):\n",
    "    # 筛选出当前调仓时间点 t 的所有币种数据（截面数据）\n",
    "    current_snapshot = bt_df[bt_df['dt'] == t]\n",
    "    if current_snapshot.empty:\n",
    "        continue\n",
    "\n",
    "    # 找出 predicted_label 最高（最好）和最低（最差）的币种\n",
    "    # 使用 .idxmax() 和 .idxmin() 找到索引，再用 .loc[] 提取 symbol_enc\n",
    "    long_symbol = current_snapshot.loc[current_snapshot['predicted_prob'].idxmax(), 'symbol_enc']\n",
    "    short_symbol = current_snapshot.loc[current_snapshot['predicted_prob'].idxmin(), 'symbol_enc']\n",
    "\n",
    "    rebalance_signals[t] = {'long': long_symbol, 'short': short_symbol}\n",
    "\n",
    "# --- 构建调仓周期内的持仓和计算周期收益 ---\n",
    "# 存储每个调仓周期的策略收益\n",
    "period_returns = []\n",
    "\n",
    "# 初始化当前持仓，确保从第一个调仓点开始生效\n",
    "current_long_symbol = None\n",
    "current_short_symbol = None\n",
    "\n",
    "# 遍历每个调仓周期\n",
    "for i in tqdm(range(len(rebalance_times)), desc=\"Calculating Period Returns\"):\n",
    "    start_time = rebalance_times[i]\n",
    "    # 确定当前调仓周期结束时间\n",
    "    end_time = rebalance_times[i+1] if i + 1 < len(rebalance_times) else timestamps_sorted[-1]\n",
    "\n",
    "    # 从 rebalance_signals 获取当前周期的多空币种\n",
    "    if start_time in rebalance_signals:\n",
    "        current_long_symbol = rebalance_signals[start_time]['long']\n",
    "        current_short_symbol = rebalance_signals[start_time]['short']\n",
    "    else:\n",
    "        # 如果当前调仓点没有信号（不应发生），则沿用上一个周期的头寸或保持空仓\n",
    "        # 这里为了简化，假设如果有信号就会找到，没有则保持上一个有效头寸\n",
    "        # 如果需要严格空仓，可以在这里设置 current_long_symbol = None, current_short_symbol = None\n",
    "        pass\n",
    "\n",
    "    # 如果没有有效头寸，跳过此周期（收益为0）\n",
    "    if current_long_symbol is None or current_short_symbol is None:\n",
    "        period_returns.append({'dt': start_time, 'strategy_log_return': 0.0})\n",
    "        continue\n",
    "\n",
    "    # 筛选出当前调仓周期内的所有数据\n",
    "    # 包括起始时间点（进行调仓），但不包括结束时间点（结算）\n",
    "    period_data = bt_df[(bt_df['dt'] >= start_time) & (bt_df['dt'] <= end_time)].copy()\n",
    "\n",
    "    # 获取多头和空头币种在该周期开始和结束时的价格\n",
    "    long_start_px = period_data[(period_data['dt'] == start_time) & (period_data['symbol_enc'] == current_long_symbol)]['px'].iloc[0]\n",
    "    long_end_px = period_data[(period_data['dt'] == end_time) & (period_data['symbol_enc'] == current_long_symbol)]['px'].iloc[0]\n",
    "\n",
    "    short_start_px = period_data[(period_data['dt'] == start_time) & (period_data['dt'] <= end_time) & (period_data['symbol_enc'] == current_short_symbol)]['px'].iloc[0]\n",
    "    short_end_px = period_data[(period_data['dt'] == end_time) & (period_data['dt'] <= end_time) & (period_data['symbol_enc'] == current_short_symbol)]['px'].iloc[0]\n",
    "\n",
    "    # 计算多头和空头在该周期内的对数收益率\n",
    "    long_log_ret = np.log(long_end_px) - np.log(long_start_px)\n",
    "    short_log_ret = np.log(short_end_px) - np.log(short_start_px)\n",
    "\n",
    "    # 计算策略在该周期内的总对数收益（多头收益 + 空头收益的绝对值）\n",
    "    # 假设各持仓权重相等，所以是 (多头收益 - 空头收益) / 2\n",
    "    # 或者说，做多一个，做空一个，组合总收益\n",
    "    strategy_period_log_return = (long_log_ret - short_log_ret) / 2 # 平均对冲策略\n",
    "\n",
    "    period_returns.append({\n",
    "        'dt': start_time,\n",
    "        'strategy_log_return': strategy_period_log_return\n",
    "    })\n",
    "\n",
    "# 将周期收益转换为 DataFrame\n",
    "strategy_returns_df = pd.DataFrame(period_returns).set_index('dt')\n",
    "strategy_returns_series = strategy_returns_df['strategy_log_return']\n",
    "\n",
    "# --- 绩效指标函数 ---\n",
    "def perf_stats(return_series, periods_per_year):\n",
    "    \"\"\"\n",
    "    计算并返回策略的绩效统计数据。\n",
    "    return_series: 每个周期的对数收益率序列。\n",
    "    periods_per_year: 一年内有多少个这样的周期（用于年化）。\n",
    "    \"\"\"\n",
    "    if return_series.empty:\n",
    "        return {\n",
    "            'Cumulative Return': np.nan, 'Annualized Return': np.nan,\n",
    "            'Annualized Volatility': np.nan, 'Sharpe Ratio': np.nan, 'Max Drawdown': np.nan\n",
    "        }\n",
    "\n",
    "    cum_ret = return_series.cumsum().apply(np.exp) # 对数收益累加后转回普通收益\n",
    "    total_return = cum_ret.iloc[-1] - 1 # 累计普通收益\n",
    "\n",
    "    # 年化收益率 (几何平均)\n",
    "    num_periods = len(return_series)\n",
    "    if num_periods > 0:\n",
    "        ann_return = (cum_ret.iloc[-1])**(periods_per_year / num_periods) - 1\n",
    "    else:\n",
    "        ann_return = np.nan\n",
    "\n",
    "    ann_vol = return_series.std() * np.sqrt(periods_per_year) # 年化波动率\n",
    "    sharpe = ann_return / ann_vol if ann_vol > 0 else np.nan\n",
    "\n",
    "    # 最大回撤 (基于普通收益)\n",
    "    running_max = cum_ret.cummax()\n",
    "    drawdown = (cum_ret - running_max) / running_max\n",
    "    max_dd = drawdown.min()\n",
    "    return {\n",
    "        'Cumulative Return': total_return,\n",
    "        'Annualized Return': ann_return,\n",
    "        'Annualized Volatility': ann_vol,\n",
    "        'Sharpe Ratio': sharpe,\n",
    "        'Max Drawdown': max_dd\n",
    "    }\n",
    "\n",
    "# --- 计算和展示绩效 ---\n",
    "# 假设每个时间戳是10分钟，N_INTERVAL=1000，则一个周期是 1000 * 10分钟 = 10000 分钟\n",
    "# 一年大约有 52560 个10分钟的间隔 (365天 * 24小时 * 6个10分钟/小时)\n",
    "# 则一年大约有 52560 / 1000 = 52.56 个 N_INTERVAL 周期\n",
    "periods_per_year_for_annualization = (365 * 24 * 60) / (N_INTERVAL * 10) # 10分钟一档\n",
    "# 或者更直接： (总时间戳数 / N_INTERVAL) / 总年数\n",
    "\n",
    "stats = perf_stats(strategy_returns_series, periods_per_year_for_annualization)\n",
    "print(\"\\n--- Strategy Performance Statistics ---\")\n",
    "print(pd.Series(stats))\n",
    "\n",
    "# --- 绘制累计收益曲线 ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "# 对数收益累加，然后取指数，得到累计普通收益曲线\n",
    "strategy_returns_series.cumsum().apply(np.exp).plot()\n",
    "plt.title(f\"Long-Short Strategy Cumulative Return (Rebalance every {N_INTERVAL} bars)\")\n",
    "plt.xlabel(\"Rebalance Timestamp\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc962a-f44c-4863-a791-640a6af051b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns # 用于绘制热力图\n",
    "\n",
    "# --- 假设 full_df 已经包含以下列 ---\n",
    "# - 'timestamp'（时间戳，可转为 datetime）\n",
    "# - 'symbol_enc'（币种编码）\n",
    "# - 'px'（当期价格）\n",
    "# - 'predicted_prob'（模型预测的概率，越大表示越好）\n",
    "\n",
    "# --- 配置参数 ---\n",
    "# 注意：你的原始代码中 bt_df = full_df，这里假设 full_df 已经被传入或定义\n",
    "# 如果 full_df 是一个未定义的变量，你需要在这里定义或加载它\n",
    "# 例如：bt_df = pd.read_csv('your_data.csv')\n",
    "bt_df = full_df\n",
    "\n",
    "# 这里N是一个未定义的变量，你需要给它一个具体的值，例如 100\n",
    "# N = 100\n",
    "N_INTERVAL = N * 1 # 调仓周期，每 N_INTERVAL 个时间戳调仓一次\n",
    "TRANSACTION_COST_PER_TRADE = 0.0007 # 每单位交易的费用，这里是一个更通用的值\n",
    "\n",
    "# 新增：单币种止损参数\n",
    "STOP_LOSS_PERCENT = 0.15 # 单币种止损百分比，例如 0.15 表示 15% 的亏损止损\n",
    "\n",
    "# ======================================================================\n",
    "# ===== 权重计算函数 =====\n",
    "# ======================================================================\n",
    "\n",
    "n_beta = 5.0\n",
    "def dollar_neutralize(weights):\n",
    "    \"\"\"\n",
    "    保证多空资金平衡：多头资金=空头资金=0.5\n",
    "    \"\"\"\n",
    "    long_sum = weights[weights > 0].sum()\n",
    "    short_sum = -weights[weights < 0].sum()\n",
    "    if long_sum == 0 or short_sum == 0:\n",
    "        return weights / np.sum(np.abs(weights))\n",
    "    w = weights.copy()\n",
    "    w[w > 0] /= long_sum\n",
    "    w[w < 0] /= short_sum\n",
    "    return w / 2  # 多头合计=0.5，空头合计=-0.5\n",
    "\n",
    "def exp_weights(scores, beta=5.0):\n",
    "    \"\"\"\n",
    "    根据预测分数计算指数加权，并进行资金中性化\n",
    "    \"\"\"\n",
    "    pos = np.exp(beta * np.clip(scores, 0, None))\n",
    "    neg = -np.exp(beta * np.clip(-scores, 0, None))\n",
    "    w = pos + neg\n",
    "    return dollar_neutralize(w)\n",
    "\n",
    "# --- 数据预处理 ---\n",
    "bt_df['dt'] = pd.to_datetime(bt_df['timestamp'], unit='ms')\n",
    "bt_df = bt_df.sort_values(['dt', 'symbol_enc']).reset_index(drop=True)\n",
    "timestamps_sorted = bt_df['dt'].drop_duplicates().sort_values().to_list()\n",
    "rebalance_times = timestamps_sorted[::N_INTERVAL]\n",
    "\n",
    "# --- 识别多空信号（在每个调仓时间点）---\n",
    "rebalance_weights = {}\n",
    "for t in tqdm(rebalance_times, desc=\"Calculating Rebalance Weights\"):\n",
    "    current_snapshot = bt_df[bt_df['dt'] == t].copy()\n",
    "    if current_snapshot.empty:\n",
    "        rebalance_weights[t] = {}\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        current_snapshot.set_index('symbol_enc', inplace=True)\n",
    "        # 核心改动：使用 exp_weights\n",
    "        weights = exp_weights(current_snapshot['predicted_prob'], beta=n_beta)\n",
    "        rebalance_weights[t] = weights.to_dict()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating weights at {t}: {e}\")\n",
    "        rebalance_weights[t] = {}\n",
    "\n",
    "# --- 构建调仓周期内的持仓和计算周期收益 ---\n",
    "period_results = []\n",
    "all_positions_by_period = []\n",
    "\n",
    "current_weights = {} # {symbol: weight}\n",
    "\n",
    "for i in tqdm(range(len(rebalance_times)), desc=\"Calculating Period Returns\"):\n",
    "    start_time = rebalance_times[i]\n",
    "    end_time = rebalance_times[i+1] if i + 1 < len(rebalance_times) else timestamps_sorted[-1]\n",
    "\n",
    "    # 获取当前周期的数据快照\n",
    "    period_data = bt_df[(bt_df['dt'] >= start_time) & (bt_df['dt'] <= end_time)].copy()\n",
    "    if period_data.empty:\n",
    "        continue\n",
    "\n",
    "    # 获取当前时间点（周期开始）的价格快照\n",
    "    current_moment_prices = bt_df[bt_df['dt'] == start_time].set_index('symbol_enc')['px'].to_dict()\n",
    "\n",
    "    # --- Step 1: 检查并处理单币种止损及联动调整 ---\n",
    "    current_period_transaction_cost = 0.0\n",
    "    long_positions_to_close = {}\n",
    "    short_positions_to_close = {}\n",
    "\n",
    "    # 遍历现有持仓检查止损\n",
    "    for symbol, weight in list(current_weights.items()):\n",
    "        open_px = current_moment_prices.get(symbol)\n",
    "        if open_px is None or weight == 0:\n",
    "            continue\n",
    "\n",
    "        symbol_data = period_data[period_data['symbol_enc'] == symbol]\n",
    "        if symbol_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # 简化处理：使用周期内最高/最低价判断是否触及止损\n",
    "        min_px = symbol_data['px'].min()\n",
    "        max_px = symbol_data['px'].max()\n",
    "\n",
    "        triggered_stop_loss = False\n",
    "        if weight > 0 and (open_px - min_px) / open_px >= STOP_LOSS_PERCENT: # 多头止损\n",
    "            triggered_stop_loss = True\n",
    "        elif weight < 0 and (max_px - open_px) / open_px >= STOP_LOSS_PERCENT: # 空头止损\n",
    "            triggered_stop_loss = True\n",
    "        \n",
    "        if triggered_stop_loss:\n",
    "            print(f\"Stop-loss triggered for {symbol} ({'LONG' if weight > 0 else 'SHORT'}) at {start_time.strftime('%Y-%m-%d %H:%M')}.\")\n",
    "            \n",
    "            if weight > 0:\n",
    "                long_positions_to_close[symbol] = weight\n",
    "            else:\n",
    "                short_positions_to_close[symbol] = weight\n",
    "            \n",
    "            # 直接平仓，并将平仓费用计入\n",
    "            current_weights[symbol] = 0.0\n",
    "            current_period_transaction_cost += abs(weight) * TRANSACTION_COST_PER_TRADE\n",
    "\n",
    "    # --- 联动调整剩余仓位以恢复资金中性 ---\n",
    "    \n",
    "    # 止损多头仓位，按比例减小所有空头仓位\n",
    "    if long_positions_to_close:\n",
    "        total_long_loss_weight = sum(long_positions_to_close.values())\n",
    "        total_short_weight = -sum(w for w in current_weights.values() if w < 0)\n",
    "        \n",
    "        if total_short_weight > 0:\n",
    "            adjustment_ratio = total_long_loss_weight / total_short_weight\n",
    "            print(f\"  -> Linkage: Adjusting short positions by ratio {adjustment_ratio:.2%}.\")\n",
    "            for symbol, weight in list(current_weights.items()):\n",
    "                if weight < 0:\n",
    "                    new_weight = weight * (1 - adjustment_ratio)\n",
    "                    current_period_transaction_cost += abs(new_weight - weight) * TRANSACTION_COST_PER_TRADE\n",
    "                    current_weights[symbol] = new_weight\n",
    "\n",
    "    # 止损空头仓位，按比例减小所有多头仓位\n",
    "    if short_positions_to_close:\n",
    "        total_short_loss_weight = -sum(short_positions_to_close.values())\n",
    "        total_long_weight = sum(w for w in current_weights.values() if w > 0)\n",
    "        \n",
    "        if total_long_weight > 0:\n",
    "            adjustment_ratio = total_short_loss_weight / total_long_weight\n",
    "            print(f\"  -> Linkage: Adjusting long positions by ratio {adjustment_ratio:.2%}.\")\n",
    "            for symbol, weight in list(current_weights.items()):\n",
    "                if weight > 0:\n",
    "                    new_weight = weight * (1 - adjustment_ratio)\n",
    "                    current_period_transaction_cost += abs(new_weight - weight) * TRANSACTION_COST_PER_TRADE\n",
    "                    current_weights[symbol] = new_weight\n",
    "\n",
    "    # --- Step 2: 根据调仓信号更新持仓并计算交易费用 ---\n",
    "    # 获取本周期的目标权重\n",
    "    target_weights = rebalance_weights.get(start_time, {})\n",
    "    \n",
    "    # 计算从当前持仓（已经过止损调整）到目标持仓的交易成本\n",
    "    all_symbols = set(current_weights.keys()) | set(target_weights.keys())\n",
    "    for symbol in all_symbols:\n",
    "        old_weight = current_weights.get(symbol, 0)\n",
    "        new_weight = target_weights.get(symbol, 0)\n",
    "        current_period_transaction_cost += abs(new_weight - old_weight) * TRANSACTION_COST_PER_TRADE\n",
    "    \n",
    "    # 更新持仓权重\n",
    "    current_weights = target_weights.copy()\n",
    "    \n",
    "    # --- Step 3: 计算本周期的策略收益 (毛收益) ---\n",
    "    gross_period_log_return = 0.0\n",
    "    if not current_weights:\n",
    "        gross_period_log_return = 0.0\n",
    "    else:\n",
    "        end_prices = period_data[period_data['dt'] == end_time].set_index('symbol_enc')['px'].to_dict()\n",
    "        start_prices = period_data[period_data['dt'] == start_time].set_index('symbol_enc')['px'].to_dict()\n",
    "        \n",
    "        for symbol, weight in current_weights.items():\n",
    "            start_px = start_prices.get(symbol)\n",
    "            end_px = end_prices.get(symbol)\n",
    "            if start_px and end_px:\n",
    "                log_return_per_symbol = np.log(end_px) - np.log(start_px)\n",
    "                gross_period_log_return += weight * log_return_per_symbol\n",
    "            \n",
    "    net_period_log_return = gross_period_log_return - current_period_transaction_cost\n",
    "\n",
    "    # --- 记录当前调仓周期所有品种的持仓状态 (用于热力图) ---\n",
    "    all_unique_symbols = bt_df['symbol_enc'].unique()\n",
    "    current_period_positions_for_heatmap = {symbol: 0.0 for symbol in all_unique_symbols}\n",
    "    for symbol, weight in current_weights.items():\n",
    "        current_period_positions_for_heatmap[symbol] = weight\n",
    "\n",
    "    all_positions_by_period.append({'dt': start_time, **current_period_positions_for_heatmap})\n",
    "\n",
    "    period_results.append({\n",
    "        'dt': start_time,\n",
    "        'gross_strategy_log_return': gross_period_log_return,\n",
    "        'transaction_cost': current_period_transaction_cost, # 本周期总费用\n",
    "        'net_strategy_log_return': net_period_log_return,\n",
    "        'num_long_positions': sum(1 for w in current_weights.values() if w > 0),\n",
    "        'num_short_positions': sum(1 for w in current_weights.values() if w < 0)\n",
    "    })\n",
    "\n",
    "# 将周期结果转换为 DataFrame\n",
    "strategy_results_df = pd.DataFrame(period_results).set_index('dt')\n",
    "\n",
    "# --- 准备持仓热力图数据 ---\n",
    "positions_heatmap_df = pd.DataFrame(all_positions_by_period).set_index('dt')\n",
    "positions_heatmap_df = positions_heatmap_df[sorted(positions_heatmap_df.columns)]\n",
    "\n",
    "\n",
    "# --- 绩效指标函数 (与之前相同) ---\n",
    "def perf_stats(return_series, periods_per_year):\n",
    "    if return_series.empty:\n",
    "        return {\n",
    "            'Cumulative Return': np.nan, 'Annualized Return': np.nan,\n",
    "            'Annualized Volatility': np.nan, 'Sharpe Ratio': np.nan, 'Max Drawdown': np.nan\n",
    "        }\n",
    "\n",
    "    cum_ret = return_series.cumsum().apply(np.exp)\n",
    "    total_return = cum_ret.iloc[-1] - 1\n",
    "\n",
    "    num_periods = len(return_series)\n",
    "    if num_periods > 0:\n",
    "        ann_return = (cum_ret.iloc[-1])**(periods_per_year / num_periods) - 1\n",
    "    else:\n",
    "        ann_return = np.nan\n",
    "\n",
    "    ann_vol = return_series.std() * np.sqrt(periods_per_year)\n",
    "    sharpe = ann_return / ann_vol if ann_vol > 0 else np.nan\n",
    "\n",
    "    running_max = cum_ret.cummax()\n",
    "    drawdown = (cum_ret - running_max) / running_max\n",
    "    max_dd = drawdown.min()\n",
    "    return {\n",
    "        'Cumulative Return': total_return,\n",
    "        'Annualized Return': ann_return,\n",
    "        'Annualized Volatility': ann_vol,\n",
    "        'Sharpe Ratio': sharpe,\n",
    "        'Max Drawdown': max_dd\n",
    "    }\n",
    "\n",
    "# --- 计算和展示绩效 ---\n",
    "periods_per_year_for_annualization = (365 * 24 * 60) / (N_INTERVAL * 10) # 假设10ms一个bar\n",
    "\n",
    "print(\"\\n--- Strategy Performance Statistics (Gross) ---\")\n",
    "gross_stats = perf_stats(strategy_results_df['gross_strategy_log_return'], periods_per_year_for_annualization)\n",
    "print(pd.Series(gross_stats))\n",
    "\n",
    "print(\"\\n--- Strategy Performance Statistics (Net of Costs) ---\")\n",
    "net_stats = perf_stats(strategy_results_df['net_strategy_log_return'], periods_per_year_for_annualization)\n",
    "print(pd.Series(net_stats))\n",
    "\n",
    "print(f\"\\nTotal Transaction Cost (Sum of individual costs): {strategy_results_df['transaction_cost'].sum():.6f}\")\n",
    "\n",
    "# --- 绘制图表 (使用低饱和度配色) ---\n",
    "fig, axes = plt.subplots(4, 1, figsize=(16, 22), sharex=False, gridspec_kw={'height_ratios': [0.35, 0.2, 0.2, 0.25]})\n",
    "\n",
    "# 定义低饱和度颜色\n",
    "COLOR_GROSS_RETURN = sns.color_palette(\"Paired\")[1]\n",
    "COLOR_NET_RETURN = sns.color_palette(\"Paired\")[3]\n",
    "COLOR_TRANSACTION_COST = sns.color_palette(\"Paired\")[5]\n",
    "COLOR_LONG_POSITIONS = sns.color_palette(\"Paired\")[7]\n",
    "COLOR_SHORT_POSITIONS = sns.color_palette(\"Paired\")[9]\n",
    "\n",
    "# 热力图颜色映射 (低饱和度 RdBu)\n",
    "HEATMAP_CMAP = sns.diverging_palette(240, 10, as_cmap=True, s=70, l=60, sep=1)\n",
    "\n",
    "# 设置图表背景风格 (可选，例如设置为白色背景)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# 子图1: 累计收益 (毛收益 vs. 净收益)\n",
    "strategy_results_df['gross_strategy_log_return'].cumsum().apply(np.exp).plot(ax=axes[0], label='Cumulative Gross Return', color=COLOR_GROSS_RETURN)\n",
    "strategy_results_df['net_strategy_log_return'].cumsum().apply(np.exp).plot(ax=axes[0], label='Cumulative Net Return', color=COLOR_NET_RETURN)\n",
    "axes[0].set_title(f\"Cumulative Strategy Returns (Rebalance every {N_INTERVAL} bars, Exponential Weighted)\")\n",
    "axes[0].set_ylabel(\"Cumulative Return\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, linestyle=':', alpha=0.7)\n",
    "axes[0].set_xlabel(\"\")\n",
    "\n",
    "# 子图2: 累计手续费消耗\n",
    "strategy_results_df['transaction_cost'].cumsum().plot(ax=axes[1], label='Cumulative Transaction Cost', color=COLOR_TRANSACTION_COST)\n",
    "axes[1].set_title(\"Cumulative Transaction Cost Over Time\")\n",
    "axes[1].set_ylabel(\"Total Cost\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, linestyle=':', alpha=0.7)\n",
    "axes[1].set_xlabel(\"\")\n",
    "\n",
    "# 子图3: 持仓数量变化 (保留，作为快速概览)\n",
    "strategy_results_df['num_long_positions'].plot(ax=axes[2], label='Number of Long Positions', color=COLOR_LONG_POSITIONS, drawstyle='steps-post')\n",
    "strategy_results_df['num_short_positions'].plot(ax=axes[2], label='Number of Short Positions', color=COLOR_SHORT_POSITIONS, drawstyle='steps-post')\n",
    "axes[2].set_title(\"Number of Long and Short Positions Over Time (Overview)\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_xlabel(\"\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, linestyle=':', alpha=0.7)\n",
    "axes[2].set_ylim(bottom=0)\n",
    "\n",
    "# 子图4: 详细持仓热力图\n",
    "if not positions_heatmap_df.empty:\n",
    "    # 限制热力图只显示部分品种，避免图表过大\n",
    "    unique_symbols = positions_heatmap_df.columns\n",
    "    if len(unique_symbols) > 50:\n",
    "        # 只显示前50个品种\n",
    "        positions_heatmap_df_display = positions_heatmap_df.iloc[:, :50]\n",
    "    else:\n",
    "        positions_heatmap_df_display = positions_heatmap_df\n",
    "\n",
    "    sns.heatmap(\n",
    "        positions_heatmap_df_display.T,\n",
    "        cmap=HEATMAP_CMAP,\n",
    "        cbar_kws={'label': 'Position Weight'},\n",
    "        ax=axes[3],\n",
    "        yticklabels=True,\n",
    "        xticklabels=True,\n",
    "        linewidths=0.5,\n",
    "        linecolor='lightgray'\n",
    "    )\n",
    "    axes[3].set_title(\"Detailed Position Weight Heatmap (Per Symbol)\")\n",
    "    axes[3].set_xlabel(\"Rebalance Timestamp\")\n",
    "    axes[3].set_ylabel(\"Symbol\")\n",
    "    \n",
    "    # 调整x轴刻度标签，避免重叠\n",
    "    num_ticks = 10 \n",
    "    if len(positions_heatmap_df_display.index) > num_ticks:\n",
    "        tick_interval = len(positions_heatmap_df_display.index) // num_ticks\n",
    "        axes[3].set_xticks(np.arange(0, len(positions_heatmap_df_display.index), tick_interval))\n",
    "        axes[3].set_xticklabels(positions_heatmap_df_display.index[::tick_interval].strftime('%Y-%m-%d %H:%M'))\n",
    "    else:\n",
    "        axes[3].set_xticklabels(positions_heatmap_df_display.index.strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8cc31-5c30-4e89-806f-6afa99d98633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def save_tabnet_checkpoint(\n",
    "    model,\n",
    "    base_save_dir: str,\n",
    "    model_params: dict,\n",
    "    feature_names: list[str],\n",
    "    training_meta: dict,\n",
    "    unique_id: str = None,\n",
    "):\n",
    "    if unique_id is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "        save_dir = os.path.join(base_save_dir, f\"tabnet_{timestamp}\")\n",
    "    else:\n",
    "        save_dir = os.path.join(base_save_dir, unique_id)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 保存模型\n",
    "    model_path = os.path.join(save_dir, \"tabnet_model\")\n",
    "    model.save_model(model_path)\n",
    "\n",
    "    # 保存模型参数和元信息\n",
    "    config_path = os.path.join(save_dir, \"model_metadata.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"model_params\": model_params,\n",
    "            \"meta_info\": training_meta,\n",
    "        }, f, indent=4)\n",
    "\n",
    "    # 保存辅助对象\n",
    "    aux_path = os.path.join(save_dir, \"auxiliary.pkl\")\n",
    "    with open(aux_path, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"feature_names\": feature_names,\n",
    "        }, f)\n",
    "\n",
    "    print(f\"✅ 模型和元信息已保存到: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d3517-bdbf-49c6-afc6-187580341a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_cols, cat_idxs, cat_dims)\n",
    "N_INTERVAL = N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c3d46-43a9-43d7-b3c5-4b41cc1dc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(symbol_to_id.keys())\n",
    "for sym, df in symbol_to_id.items():\n",
    "    print(sym)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e28a1-f651-4c70-bb57-f9d19f143a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_np_to_builtin(d):\n",
    "    return {str(k): int(v) for k, v in d.items()}\n",
    "\n",
    "def convert_dict_keys_to_str_and_values_to_builtin(d):\n",
    "    new_d = {}\n",
    "    for k, v in d.items():\n",
    "        # key 可能是 np.int64\n",
    "        new_key = int(k) if isinstance(k, (np.integer,)) else str(k)\n",
    "        # value 可能是 np.str_\n",
    "        new_val = str(v) if isinstance(v, (np.str_,)) else v\n",
    "        new_d[new_key] = new_val\n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b9cef-b335-4259-b953-409b1f427542",
   "metadata": {},
   "outputs": [],
   "source": [
    "a  =convert_dict_np_to_builtin(symbol_to_id)\n",
    "print(a[\"BNBUSDT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de7bb7-0ced-44bc-a5e3-283d487faeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = convert_dict_keys_to_str_and_values_to_builtin(id_to_symbol)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1c4f4-83e5-4bab-9644-7a11bbafbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# save_tabnet_checkpoint(\n",
    "#     model=tabnet,\n",
    "#     base_save_dir=\"./saved_models/tabnet_crosec\",\n",
    "#     model_params=params,\n",
    "#     feature_names=feature_cols,\n",
    "#     training_meta={\n",
    "#         \"symbol_to_id\": convert_dict_np_to_builtin(symbol_to_id),\n",
    "#         \"id_to_symbol\": convert_dict_keys_to_str_and_values_to_builtin(id_to_symbol),\n",
    "#         \"train_n_week\": n_train_weeks,\n",
    "#         \"fit_params\": init_fit_params,\n",
    "#         \"label_window\": N,\n",
    "#         \"saved_timestamp\": str(pd.Timestamp.now()),\n",
    "#         \"feat_cal_window\": int(feat_cal_window),\n",
    "#         \"feat_norm_window\": feat_norm_window,\n",
    "#         \"feat_norm_rolling_mean_window\": feat_norm_rolling_mean_window,\n",
    "#         \"n_interval\": N_INTERVAL,\n",
    "#         \"n_beta\": n_beta,\n",
    "#         \"sl_percent\": STOP_LOSS_PERCENT,\n",
    "#     },\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EI_Lab)",
   "language": "python",
   "name": "ei_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
